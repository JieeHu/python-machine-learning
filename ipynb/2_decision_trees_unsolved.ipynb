{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 2  - Decision trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursion\n",
    "\n",
    "A recursive function is a function that calls itself until some base case is reached. The base case is some condition we check with every call to the function to make sure it still makes sense to call itself. Without the base case the recursion would continue infinitely.\n",
    "\n",
    "Recursion is often explained by referring to Russian nesting dolls. Each time you open a doll, another doll is inside, this continues until you reach the smallest doll (the base case). Without knowing how many dolls there are we know how to solve the task of opening all the dolls, as we simply keep calling the open *'function'* until we reach the last doll.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d2/Russian-Matroshka_no_bg.jpg\" width=\"30%\">\n",
    "\n",
    "An example of a problem we can solve using a recursive function is calculating the factorial. The base case is that if ```n == 1``` we no longer need to calculate the factorial, as here we know the answer, and otherwise we calculate the answer by calculating the factorial for ```n-1```, until we reach 1. In the cell below the ```factorial``` function is given, with print statements to show whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the factorial for 5 let's try 4\n",
      "I don't know the factorial for 4 let's try 3\n",
      "I don't know the factorial for 3 let's try 2\n",
      "I don't know the factorial for 2 let's try 1\n",
      "This I know! (the base case)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 1:\n",
    "        print(\"This I know! (the base case)\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"I don't know the factorial for\", n, \"let's try\", n-1)\n",
    "        return n * factorial(n-1)\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice recursion.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Write a recursive function ``rec_sum`` which takes a list of numbers and returns the sum of that list. \n",
    "\n",
    "**Hint:** Remember that you can use the a colon to select a part of a list. For example ```a[2:]``` returns all but the first two elements from the list ```a```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "rec_sum([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the function in exercise 1 is not the most useful recursive function, and it would be easier solved with just a loop. But it might help you get started thinking about how it works. Let's see how it can be more useful.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In the cell below you are given a list which contains a nested list which contains another nested list, etc. You do not know how many levels of nesting lists there are, all you know that the last list contains a number. Write a recursive function which prints this number by searching through the list.\n",
    "\n",
    "As an advanced exercise you can also try to keep track of how many levels you had to descend in order to reach the final answer.\n",
    "\n",
    "**Hint:** You can check if something is a list using the ```isinstance``` function: ```isinstance([1,2,3], list)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 37)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested = [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[13]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n",
    "\n",
    "def ...\n",
    "    \n",
    "search(nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree structures\n",
    "\n",
    "Recursion is very useful when dealing with tree structures, as we often do not know how deep the tree is. All we can see is if the node we are currently looking at has any children, and if it does we can try to visit those, and repeat this.\n",
    "\n",
    "Different from the examples above, a tree splits up into branches, which means we're not doing one, but two (and sometimes even more) recursive calls everytime we are going down a level. Decision trees are binary trees: this means that every node has either 0 or 2 children. If it has 0 then it is a leaf node. The figure below is annotated with some of the terminology used when talking about trees.\n",
    "\n",
    "\n",
    "<img src=\"https://www.tutorialspoint.com/data_structures_algorithms/images/binary_tree.jpg\" alt=\"Tree structure\" width=\"50%\">\n",
    "\n",
    "In the cell below I have defined a class called ```Node```, which we can use to construct a Decision tree. The node stores references to its children (left and right), it stores which attributes (feature) of our dataset we want to apply the decision to, the value we want to compare our feature with, and finally what the majority class is at that node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in a (binary) decision tree\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialiser of the class\"\"\"\n",
    "        self.left = None # left child\n",
    "        self.right = None # right child\n",
    "        self.attribute = None # column on which we decide\n",
    "        self.value = None # value to check against\n",
    "        self.majority = None # Majority class at this label\n",
    "    \n",
    "    def isleaf(self):\n",
    "        \"\"\"Helper function to check if the current node is a leaf\"\"\"\n",
    "        if self.left == None and self.right == None:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def question(self, attribute, value):\n",
    "        \"\"\" Helper function to add question to node.\n",
    "        \"\"\"\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self, depth=1):\n",
    "        \"\"\" You can ignore this function, \n",
    "        but basically it helps print the node in a human-readable manner \"\"\"\n",
    "        if self.isleaf():\n",
    "            return \"Predict: \\\"{:s}\\\"\".format(self.majority)\n",
    "        else:\n",
    "            s = \"if features[{:d}] == \\\"{:s}\\\" then:\\n {:s} \\n{:s}else:\\n {:s}\"\n",
    "            return s.format(self.attribute, \n",
    "                            self.value, \n",
    "                            \"\\t\" * depth+self.left.__str__(depth+1),\n",
    "                            \"\\t\" * (depth-1),\n",
    "                            \"\\t\" * depth+self.right.__str__(depth+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we use the Node class?  To illustrate this you'll you'll find an example of a made-up tree below, applied to a number of objects (i.e., fruits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if features[2] == \"Round\" then:\n",
      " \tif features[1] == \"Red\" then:\n",
      " \t\tPredict: \"Apple\" \n",
      "\telse:\n",
      " \t\tPredict: \"Lime\" \n",
      "else:\n",
      " \tPredict: \"Banana\"\n"
     ]
    }
   ],
   "source": [
    "# Create a new node and store it in the root variable\n",
    "root = Node()\n",
    "# Specify the decision we want to take at this node\n",
    "# In this case we want to see if feature 2 contains the value Round\n",
    "# We can use the question function to specify the attribute-value pair to be used as a question:\n",
    "root.question(2, \"Round\")\n",
    "# Create a new node, which we'll visit if the object is round.\n",
    "root.left = Node()\n",
    "# Create another node, we'll go here if it is not round.\n",
    "root.right = Node()\n",
    "# The right node is a leaf node, if it's not round we'll predict Banana\n",
    "# Normally you'd want to determine the majority based on the data\n",
    "root.right.majority = \"Banana\"\n",
    "\n",
    "# Continue with the left node, let's see if our round object is red\n",
    "root.left.question(1, \"Red\")\n",
    "# If it is red then we'll predict Apple\n",
    "root.left.left = Node()\n",
    "root.left.left.majority = \"Apple\"\n",
    "# Otherwise it has to be a lime, as anything round and not red must be a lime. Right?!\n",
    "root.left.right = Node()\n",
    "root.left.right.majority = \"Lime\"\n",
    "# Try to extend the tree further by continuing on from here\n",
    "\n",
    "# Thanks to the __str__() function we can print the tree \n",
    "# and get the rules formatted in a humanly readable format.\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is root a leaf node? False\n",
      "Is the right child of root a leaf node? True\n",
      "\n",
      "The root looks at feature 2 and checks if its value is equal to Round\n"
     ]
    }
   ],
   "source": [
    "# Additionally we can use isleaf() to check if a node is a leaf node or not.\n",
    "print(\"Is root a leaf node?\", root.isleaf())\n",
    "\n",
    "print(\"Is the right child of root a leaf node?\", root.right.isleaf())\n",
    "print()\n",
    "\n",
    "# If we want to find out which feature the root looks at we can:\n",
    "print(\"The root looks at feature\", root.attribute, \"and checks if its value is equal to\", root.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In the example above I made up the decisions, but normally you would want to generate these based on the data. For this we'll use the weather dataset in the next cell. The objective of this dataset is to figure out if the weather conditions are such that it is nice enough to go and play outside. \n",
    "\n",
    "It has the following features, all of which are categorical.\n",
    "- outlook {sunny, overcast, rainy}\n",
    "- temperature {hot, mild, cool}\n",
    "- humidity {high, normal}\n",
    "- windy {TRUE, FALSE}\n",
    "\n",
    "And the target is:\n",
    "- Can we play outside today? {yes, no}\n",
    "\n",
    "The features are stored in ``X_train``. Each row in ``X_train`` is a different day/moment; ``y_train`` contains the label for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [['sunny', 'hot', 'high', 'FALSE'],\n",
    " ['sunny', 'hot', 'high', 'TRUE'],\n",
    " ['overcast', 'hot', 'high', 'FALSE'],\n",
    " ['rainy', 'mild', 'high', 'FALSE'],\n",
    " ['rainy', 'cool', 'normal', 'FALSE'],\n",
    " ['rainy', 'cool', 'normal', 'TRUE'],\n",
    " ['overcast', 'cool', 'normal', 'TRUE'],\n",
    " ['sunny', 'mild', 'high', 'FALSE'],\n",
    " ['sunny', 'cool', 'normal', 'FALSE'],\n",
    " ['rainy', 'mild', 'normal', 'FALSE'],\n",
    " ['sunny', 'mild', 'normal', 'TRUE'],\n",
    " ['overcast', 'mild', 'high', 'TRUE'],\n",
    " ['overcast', 'hot', 'normal', 'FALSE'],\n",
    " ['rainy', 'mild', 'high', 'TRUE']]\n",
    "\n",
    "y_train = ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some quick analysis of the distribution of the features and the label, and write a function which will be useful later on.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Write a function in the next cell called ```majority``` which takes a list of categorical values and returns the one which occurs most often.\n",
    "\n",
    "**Hint:** {'A': 5, 'B': 6}.items() returns an iterator of pairs of tuples, which you can sort using ```sorted()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority is correct!\n"
     ]
    }
   ],
   "source": [
    "def majority...\n",
    "\n",
    "if majority(y_train) == 'yes' and majority(y_train[:3]) == 'no':\n",
    "    print(\"Majority is correct!\")\n",
    "else:\n",
    "    print(\"Your majority function contains a mistake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and evaluating potential questions\n",
    "\n",
    "Now that we have a dataset we can figure out how to add questions. To do this we first need to generate the set of potential questions. Because we are dealing with features which are categorical all our questions are going to be whether the feature's value is equal to the specified value (of the form ```if temperature == 'hot'```). \n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Write a function in the next cell that takes a list of training inputs (structured like ``X_train``) as input and returns the unique values in each column. The output should be a list of sets (each set corresponding to a column).\n",
    "\n",
    "You shouldn't need a recursive function to solve this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'overcast', 'rainy', 'sunny'},\n",
       " {'cool', 'hot', 'mild'},\n",
       " {'high', 'normal'},\n",
       " {'FALSE', 'TRUE'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def questionset(X):\n",
    "\n",
    "questionset(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we determine whether a question is a good one to ask, let's figure out how to actually apply one to a dataset. Or in others words if we have a question how do we split the dataset according to the answer.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Write a function in the cell below that takes a node, a list of training examples (``X``), and a list of training targets (``y``), and returns four lists. The first containg the rows from ``X`` for which the answer to the specified question is ``False`` and the second containing the targets for those rows. The third and fourth lists should contain the same but then for the rows which give the answer ``True``.\n",
    "\n",
    "**Hints** \n",
    "- The node has a defined question, so you can use ```node.attribute``` and ```node.value``` to perform the conditional.\n",
    "- The easiest way to do this is probably by creating the four lists at the start of your function, appending to them when appropriate and then returning them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['sunny', 'hot', 'high', 'FALSE'],\n",
       "  ['sunny', 'hot', 'high', 'TRUE'],\n",
       "  ['rainy', 'mild', 'high', 'FALSE'],\n",
       "  ['rainy', 'cool', 'normal', 'FALSE'],\n",
       "  ['rainy', 'cool', 'normal', 'TRUE'],\n",
       "  ['sunny', 'mild', 'high', 'FALSE'],\n",
       "  ['sunny', 'cool', 'normal', 'FALSE'],\n",
       "  ['rainy', 'mild', 'normal', 'FALSE'],\n",
       "  ['sunny', 'mild', 'normal', 'TRUE'],\n",
       "  ['rainy', 'mild', 'high', 'TRUE']],\n",
       " ['no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no'],\n",
       " [['overcast', 'hot', 'high', 'FALSE'],\n",
       "  ['overcast', 'cool', 'normal', 'TRUE'],\n",
       "  ['overcast', 'mild', 'high', 'TRUE'],\n",
       "  ['overcast', 'hot', 'normal', 'FALSE']],\n",
       " ['yes', 'yes', 'yes', 'yes'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Node()\n",
    "root.question(0, 'overcast')\n",
    "\n",
    "def split(node, X, y):\n",
    "    \n",
    "split(root, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to make splits, we need to figure out which is the best question to ask first. In the lecture we discussed how the best decisions reduces the uncertainty the most. So let's write some functions to help us measure the uncertainty.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy is a measure of uncertainty, Entropy is calculated as follows:\n",
    "\n",
    "$I(P) = - \\sum\\limits_{i=1}^N P_i log_2(P_i)$\n",
    "\n",
    "where $P$ is a list of class probabilities (i.e., the proportion the class is present in the set). Given that for decision trees you'll be dealing with lists of labels you'll need to convert these to probabilities for each individual label. \n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "Write the ```entropy``` function in the cell below. Add tests to verify that the entropy of the list ```[0,1]``` is `1.0`, of the list ```[-1,1,2,3]``` is 2.0, and that the entropy of the first 10 examples of y_train is higher than that of the whole y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(labels):\n",
    "\n",
    "if not (entropy([0,1]) == 1.0 and\n",
    "        entropy([-1,1,2,3]) == 2.0 and\n",
    "        entropy(y_train[:10]) > entropy(y_train)):\n",
    "    print(\"Your entropy function contains a mistake!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted impurity\n",
    "\n",
    "We now need to aggregate the entropy at both the left and the right node, while weighting by the relative size of the children.\n",
    "\n",
    "$G(P) = f_{left} I(n_{left}) + f_{right} I(n_{right})$\n",
    "\n",
    "Here $n_{left}$ is the left child, and $f_{left}$ is the weight given to the left child, etc.\n",
    "\n",
    "Usually the weight  is equal to the proportion the child node has of the parent node. For example, if the parent contains $20$ instances, and after the split the left child would have 15 and the right 5, then $f_{left} = \\frac{15}{20}$ and $f_{right} = \\frac{5}{20}$.\n",
    "\n",
    "### Information gain\n",
    "\n",
    "You will often see Information Gain as a scoring function, rather than simply weighted entropy. Information gain is:\n",
    "\n",
    "$$IG(P) = I(P) - [f_{left} I(n_{left}) + f_{right} I(n_{right})]$$\n",
    "\n",
    "so it is simply the entropy of the parent node, minus the weighted entropy of the children. Since the term $I(P)$ is the same for each split, we can just ignore it and simply use the weighted entropy directly to choose the split.\n",
    "\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Implement the ``G`` function in the cell below. To verify if you have done it correctly you can split ```X_train``` and ```y_train``` using the function you wrote for exercise 5 using node A and B. If correct, the weighted impurity ``G`` using node A should be lower than when using node B.\n",
    "\n",
    "**Hint:** It should be enough to give it two lists, the list for the left child and that for the right child. As the parent list is the concatination of those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 0.7142857142857143\n",
      "B 0.8380423950607803\n"
     ]
    }
   ],
   "source": [
    "A = Node()\n",
    "A.question(0, 'overcast')\n",
    "B = Node()\n",
    "B.question(0, 'sunny')\n",
    "\n",
    "def G(left, right):\n",
    "\n",
    "L, yL, R, yR = split(A, X_train, y_train)\n",
    "print(\"A\", G(yL,yR))\n",
    "\n",
    "L, yL, R, yR = split(B, X_train, y_train)\n",
    "print(\"B\", G(yL,yR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have all the building blocks we need to start fitting a tree to a dataset. Let's give it a go!\n",
    "\n",
    "### Advanced Exercise 1\n",
    "\n",
    "Implement the ```fit(X,y)``` function below, where ``X`` is a matrix of features and y is a list of labels. It should return a tree (i.e., a instance of the Node() class).\n",
    "\n",
    "**Hints:** \n",
    "- Start by thinking of what the right base case is, and implement this.\n",
    "- Remember that you can call the ``Node.question`` function repeatedly to change the question, allowing you to test multiple questions without creating new Nodes.\n",
    "- Remember that this should be a recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if features[0] == \"overcast\" then:\n",
      " \tif features[2] == \"high\" then:\n",
      " \t\tif features[3] == \"FALSE\" then:\n",
      " \t\t\tif features[0] == \"sunny\" then:\n",
      " \t\t\t\tPredict: \"no\" \n",
      "\t\t\telse:\n",
      " \t\t\t\tPredict: \"yes\" \n",
      "\t\telse:\n",
      " \t\t\tPredict: \"yes\" \n",
      "\telse:\n",
      " \t\tif features[0] == \"sunny\" then:\n",
      " \t\t\tif features[3] == \"FALSE\" then:\n",
      " \t\t\t\tPredict: \"no\" \n",
      "\t\t\telse:\n",
      " \t\t\t\tPredict: \"yes\" \n",
      "\t\telse:\n",
      " \t\t\tPredict: \"no\" \n",
      "else:\n",
      " \tPredict: \"yes\"\n"
     ]
    }
   ],
   "source": [
    "def fit(X, y):\n",
    "    \n",
    "decision_tree = fit(X_train, y_train)\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Advanced exercise 2\n",
    "\n",
    "Once we have fitted a decision tree we would like to verify how well it works, and use it to predict the label for new samples. Implement the ```predict(tree, x)``` function in the cell below, where ```tree``` is a fitted tree, and ``x`` is one feature vector (a list). It should return a single label, either 'yes' or 'no'.\n",
    "\n",
    "**Hints:**\n",
    "- What is the base case?\n",
    "- Remember that going left or right depends on the answer to the question at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tData\t\t\tTruth\tPrediction\n",
      "['sunny', 'hot', 'high', 'FALSE'] \t no \t no\n",
      "['sunny', 'hot', 'high', 'TRUE'] \t no \t no\n",
      "['overcast', 'hot', 'high', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'high', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'cool', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'cool', 'normal', 'TRUE'] \t no \t no\n",
      "['overcast', 'cool', 'normal', 'TRUE'] \t yes \t yes\n",
      "['sunny', 'mild', 'high', 'FALSE'] \t no \t no\n",
      "['sunny', 'cool', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'normal', 'FALSE'] \t yes \t yes\n",
      "['sunny', 'mild', 'normal', 'TRUE'] \t yes \t yes\n",
      "['overcast', 'mild', 'high', 'TRUE'] \t yes \t yes\n",
      "['overcast', 'hot', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'high', 'TRUE'] \t no \t no\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, x):\n",
    "\n",
    "\n",
    "# This code applies the predict function to \n",
    "print('\\t\\tData\\t\\t\\tTruth\\tPrediction')\n",
    "for row, label in zip(X_train, y_train):\n",
    "    print(row, '\\t', label, '\\t', predict(decision_tree, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced exercise 3\n",
    "\n",
    "Visually it's quite easy to figure out how deep a tree is, but can we do it automatically? Write a recursive function in the cell below which returns the depth of the decision tree it was passed as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def depth(tree):\n",
    "  \n",
    "    \n",
    "depth(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
