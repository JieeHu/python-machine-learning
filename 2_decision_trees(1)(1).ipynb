{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decision trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion\n",
    "\n",
    "A recursive function is a function that calls itself until some base case is reached. The base case is a condition we check with every call to the function to make sure it still makes sense to call itself. Without the base case the recursion would continue infinitely.\n",
    "\n",
    "Recursion is often explained by referring to Russian nesting dolls. Each time you open a doll, another doll is inside. This continues until you reach the smallest doll (the base case). Without knowing how many dolls there are we know how to solve the task of opening all the dolls, as we simply keep calling the open *'function'* until we reach the last doll.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d2/Russian-Matroshka_no_bg.jpg\" width=\"30%\">\n",
    "\n",
    "An example of a problem we can solve using a recursive function is calculating the factorial. The base case is that if ```n == 1``` we no longer need to calculate the factorial, as here we know the answer, and otherwise we calculate the answer by calculating the factorial for ```n-1```, until we reach 1. In the cell below the ```factorial``` function is given, with print statements to show what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the factorial for 5 let's try 4\n",
      "I don't know the factorial for 4 let's try 3\n",
      "I don't know the factorial for 3 let's try 2\n",
      "I don't know the factorial for 2 let's try 1\n",
      "This I know! (the base case)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def factorial(n):\n",
    "    if n == 1:\n",
    "        print(\"This I know! (the base case)\")\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"I don't know the factorial for\", n, \"let's try\", n-1)\n",
    "        return n * factorial(n-1)\n",
    "    \n",
    "factorial(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice a bit with recursion.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Write a recursive function in the cell below which takes a list of numbers and returns the sum of that list. \n",
    "\n",
    "**Hint:** Remember that you can use the a colon to select a part of a list. For example ```a[2:]``` returns all but the first two elements from the list ```a```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rec_sum(a):\n",
    "    if len(a) == 1:\n",
    "        return a[0]\n",
    "    else:\n",
    "        return a[0] + rec_sum(a[1:])\n",
    "   \n",
    "    \n",
    "rec_sum([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the function in exercise 1 is not the most useful recursive function, and it can be easily solved with a loop. But it might help you get started thinking about how it works. Let's see how it can be more useful.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "In the cell below you are given a list, which contains a nested list, which contains another nested list, etc. You do not know how many nested lists there are, all you know that the last list contains a number. Write a recursive function which prints this number by searching through the list.\n",
    "\n",
    "Another variant is to also keep track of how many levels you had to descend in order to reach the final answer.\n",
    "\n",
    "**Hint:** You can check if something is a list using the ```isinstance``` function: ```isinstance([1,2,3], list) == True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 37)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nested = [[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[13]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]\n",
    "\n",
    "def search(a, depth=0):\n",
    "    if isinstance(a, list):\n",
    "        return search(a[0], depth+1)\n",
    "    else:\n",
    "        return a, depth\n",
    "    \n",
    "search(nested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree structures\n",
    "\n",
    "Recursive functions are very useful when dealing with tree structures,  which are recursive structures themselves. We do not know how deep the tree is. All we can see is if the node we are currently looking at has any children, and if it does we can try to visit those, and repeat this.\n",
    "\n",
    "A tree splits up into branches (child nodes). Decision trees are usually full binary trees which means that every node has either 0 or 2 children. If it has 0 then it is a leaf node. The figure below is annotated with some of the terminology used when talking about trees.\n",
    "\n",
    "\n",
    "<img src=\"https://www.tutorialspoint.com/data_structures_algorithms/images/binary_tree.jpg\" alt=\"Tree structure\" width=\"50%\">\n",
    "\n",
    "The cell below defines a class ```Node```, which we can use to construct a Decision tree. The nodes stores references to its children (left and right) and to the question (feature value) to ask about at this node, as well as the class we'll predict at this node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"A node in a binary decision tree\"\"\"\n",
    "    \n",
    "    def __init__(self, left=None, right=None, feature=None, value=None, predict=None):\n",
    "        \"\"\"Initialize the node with attributes.\"\"\"\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = feature # column in which features is stored\n",
    "        self.value = value # value to check against\n",
    "        self.predict = predict # class to predict at this node\n",
    "        \n",
    "    def isLeaf(self):\n",
    "        \"\"\"Helper function to check if the current node is a leaf\"\"\"\n",
    "        return self.left is None and self.right is None\n",
    "       \n",
    "    def __str__(self, depth=1):\n",
    "        \"\"\" You can ignore this function, \n",
    "        but basically it helps print the node in a human-readable manner \"\"\"\n",
    "        if self.isLeaf():\n",
    "            return \"Predict: \\\"{:s}\\\"\".format(self.predict)\n",
    "        else:\n",
    "            s = \"if features[{:d}] != \\\"{:s}\\\" then:\\n {:s} \\n{:s}else:\\n {:s}\"\n",
    "            return s.format(self.feature, \n",
    "                            self.value, \n",
    "                            \"\\t\" * depth+self.left.__str__(depth+1),\n",
    "                            \"\\t\" * (depth-1),\n",
    "                            \"\\t\" * depth+self.right.__str__(depth+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's illustrate the use of this class with an example of a made-up tree below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if features[2] != \"Round\" then:\n",
      " \tPredict: \"Banana\" \n",
      "else:\n",
      " \tif features[1] != \"Red\" then:\n",
      " \t\tPredict: \"Lime\" \n",
      "\telse:\n",
      " \t\tPredict: \"Apple\"\n"
     ]
    }
   ],
   "source": [
    "# We want to first ask about value Round in column at index 2.\n",
    "root = Node(feature=2, value=\"Round\",         \n",
    "            \n",
    "            # If false, in the left branch, which is a leaf node, \n",
    "            # we'll we'll predict Banana  \n",
    "            left=Node(predict=\"Banana\"),\n",
    "            \n",
    "            # If true, in the right branch we'll ask about the color Red\n",
    "            right=Node(feature=1, value=\"Red\", \n",
    "                       \n",
    "                       # Based on the answer to question about color Red, \n",
    "                       # we'll predict either Lime\n",
    "                       left=Node(predict=\"Lime\"),\n",
    "                       \n",
    "                       # or Apple\n",
    "                       right=Node(predict=\"Apple\")))\n",
    "\n",
    "# Thanks to the __str__() function we can print the tree \n",
    "# and get the rules formatted in a humanly readable format.\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is root a leaf node? False\n",
      "Is the right child of root a leaf node? False\n",
      "\n",
      "The root looks at column 2 and checks if its value is equal or not to Round\n"
     ]
    }
   ],
   "source": [
    "# Additionally we can use isleaf() to check if a node is a leaf node or not.\n",
    "print(\"Is root a leaf node?\", root.isLeaf())\n",
    "\n",
    "print(\"Is the right child of root a leaf node?\", root.right.isLeaf())\n",
    "print()\n",
    "\n",
    "# If we want to find out which column the root looks at we can:\n",
    "print(\"The root looks at column\", root.feature, \n",
    "      \"and checks if its value is equal or not to\", root.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In the example above we made up the decisions, but normally you would want to generate these based on the data. For this we'll use the weather dataset in the next cell, the objective of this dataset is to figure out if the weather conditions are such that it is nice enough to go and play outside. \n",
    "\n",
    "It has the following features, all of which are categorical.\n",
    "- outlook {sunny, overcast, rainy}\n",
    "- temperature {hot, mild, cool}\n",
    "- humidity {high, normal}\n",
    "- windy {TRUE, FALSE}\n",
    "\n",
    "And the target is:\n",
    "- Can we play outside today? {yes, no}\n",
    "\n",
    "The features are stored in X_train, each row in X_train is a different day/moment. y_train contains the label for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [['sunny', 'hot', 'high', 'FALSE'],\n",
    " ['sunny', 'hot', 'high', 'TRUE'],\n",
    " ['overcast', 'hot', 'high', 'FALSE'],\n",
    " ['rainy', 'mild', 'high', 'FALSE'],\n",
    " ['rainy', 'cool', 'normal', 'FALSE'],\n",
    " ['rainy', 'cool', 'normal', 'TRUE'],\n",
    " ['overcast', 'cool', 'normal', 'TRUE'],\n",
    " ['sunny', 'mild', 'high', 'FALSE'],\n",
    " ['sunny', 'cool', 'normal', 'FALSE'],\n",
    " ['rainy', 'mild', 'normal', 'FALSE'],\n",
    " ['sunny', 'mild', 'normal', 'TRUE'],\n",
    " ['overcast', 'mild', 'high', 'TRUE'],\n",
    " ['overcast', 'hot', 'normal', 'FALSE'],\n",
    " ['rainy', 'mild', 'high', 'TRUE']]\n",
    "\n",
    "y_train = ['no', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some quick analysis of the distribution of the features and the label, and write a function which will be useful later on.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Write a function in the next cell called ```majority``` which takes a list of categorical values and returns which occurs most often.\n",
    "\n",
    "**Hint:** {'A': 5, 'B': 6}.items() returns an iterator of pairs of tuples, which you can sort using ```sorted()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def majority(a):\n",
    "    counts = {}\n",
    "    for i in a:\n",
    "        if i in counts:\n",
    "            counts[i] += 1\n",
    "        else:\n",
    "            counts[i] = 0\n",
    "    #print(counts)\n",
    "    return sorted(counts.items(), key=lambda x: x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority is correct!\n"
     ]
    }
   ],
   "source": [
    "if majority(y_train) == 'yes' and majority(y_train[:3]) == 'no':\n",
    "    print(\"Majority is correct!\")\n",
    "else:\n",
    "    print(\"Your majority function contains a mistake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and evaluating potential decisions\n",
    "\n",
    "Now that we have a dataset we can figure out which questions to ask. To do this we first need to generate the set of potential questions. Because we are dealing with features which are categorical all our questions are going to be whether the feature's value is equal to a particular (of the form ```if temperature == 'hot'```). \n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "Write a function ```question_set``` which ```X_train``` as input and returns the unique values in each column. One way to do this is to generate a list where the nth row contains the set of unique values in the $n$th column.\n",
    "\n",
    "You shouldn't need a recursive function to solve this.\n",
    "\n",
    "**Hint:** the **set** of potential questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'overcast', 'rainy', 'sunny'},\n",
       " {'cool', 'hot', 'mild'},\n",
       " {'high', 'normal'},\n",
       " {'FALSE', 'TRUE'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def question_set(X):\n",
    "    samples = len(X)\n",
    "    features = len(X[0])\n",
    "    qset = [set() for col in range(features)]\n",
    "\n",
    "    for f in range(features):\n",
    "        for s in range(samples):\n",
    "            qset[f].add(X[s][f])\n",
    "            \n",
    "    return qset\n",
    "\n",
    "question_set(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue to determining whether a question is a good one to ask or not, let's figure out how to actually apply one to a dataset. Or in others words if we have a question how do we split the dataset according to the answer.\n",
    "\n",
    "### Exercise 5\n",
    "\n",
    "Write the function ```split``` in the cell below that takes the node defined below, ```X_train```, and ```y_train```, and returns four lists. The first containing the rows from X_train which have outlook == overcast, the second containing the labels for those rows. The third and fourth lists should contain the same but then for the rows which have a different outlook.\n",
    "\n",
    "**Hints** \n",
    "- The easiest way to do this is probably by creating the four lists at the start of your function, appending to them when appropriate and then returning them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['sunny', 'hot', 'high', 'FALSE'],\n",
       "  ['sunny', 'hot', 'high', 'TRUE'],\n",
       "  ['rainy', 'mild', 'high', 'FALSE'],\n",
       "  ['rainy', 'cool', 'normal', 'FALSE'],\n",
       "  ['rainy', 'cool', 'normal', 'TRUE'],\n",
       "  ['sunny', 'mild', 'high', 'FALSE'],\n",
       "  ['sunny', 'cool', 'normal', 'FALSE'],\n",
       "  ['rainy', 'mild', 'normal', 'FALSE'],\n",
       "  ['sunny', 'mild', 'normal', 'TRUE'],\n",
       "  ['rainy', 'mild', 'high', 'TRUE']],\n",
       " ['no', 'no', 'yes', 'yes', 'no', 'no', 'yes', 'yes', 'yes', 'no'],\n",
       " [['overcast', 'hot', 'high', 'FALSE'],\n",
       "  ['overcast', 'cool', 'normal', 'TRUE'],\n",
       "  ['overcast', 'mild', 'high', 'TRUE'],\n",
       "  ['overcast', 'hot', 'normal', 'FALSE']],\n",
       " ['yes', 'yes', 'yes', 'yes'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(feature, value, X, y):\n",
    "    yes, no = [], []\n",
    "    yes_labels, no_labels = [], []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        if X[i][feature] == value:\n",
    "            yes.append(X[i])\n",
    "            yes_labels.append(y[i])\n",
    "        else:\n",
    "            no.append(X[i])\n",
    "            no_labels.append(y[i])\n",
    "            \n",
    "    return no, no_labels, yes, yes_labels\n",
    "    \n",
    "split(0, 'overcast', X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to split the dataset based on questions, we need decide which questions to ask first. In the lecture we discussed how the best decisions reduces the uncertainty the most. So let's write some functions to help us measure the uncertainty.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy is a measure of uncertainty, Entropy is calculated as follows:\n",
    "\n",
    "$H(P) = - \\sum\\limits_{i=1}^N P_i \\log_2(P_i)$\n",
    "\n",
    "where $P$ is a list of class probabilities (i.e., the proportion the class is present in the set). Given that for decision trees you'll be dealing with lists of labels you'll need to convert these to probabilities for each individual label. \n",
    "\n",
    "### Exercise 6\n",
    "\n",
    "Write the ```entropy``` function in the cell below. Add tests to verify that the entropy of the list ```[0,1]``` is `1.0`, of the list ```[-1,1,2,3]``` is 2.0, and that the entropy of the first 10 examples of y_train is higher than that of the whole y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(labels):\n",
    "    counts = {}\n",
    "    for label in labels:\n",
    "        counts[label] = counts.get(label, 0) + 1/len(labels)\n",
    "    #print(counts)\n",
    "    return -sum([p*log2(p) for p in counts.values()])\n",
    "\n",
    "if not (entropy([0,1]) == 1.0 and\n",
    "        entropy([-1,1,2,3]) == 2.0 and\n",
    "        entropy(y_train[:10]) > entropy(y_train)):\n",
    "    print(\"Your entropy function contains a mistake!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information gain\n",
    "\n",
    "However, just measuring the entropy at a certain point in the tree isn't enough. We need to know how much the entropy goes down if we make a certain decision. For decision trees a commonly used measure is Information Gain. The information gain is calculated by subtracting the entropy of the weighted sum of the child nodes from the entropy of the parent node. Typically weighting is done by the relative size of the children.\n",
    "\n",
    "$IG(P) = H(P) - \\sum\\limits_{i=1}^N w_i~H(C_i)$\n",
    "\n",
    "Here $N$ is the number of children, $C_i$ is the ith child, and $w_i$ is the weight given to the ith child. Given that we'll be dealing with decision trees which only have a left and a right child we can also write this out to be:\n",
    "\n",
    "$IG(P) = H(P) - w_{left}~H(C_{left}) - w_{right}~H(C_{right})$\n",
    "\n",
    "Usually the weight (i.e., $w_{left}$) is equal to the proportion the child node has of the parent node. For example, if the parent contains $20$ instances, and after the decision the left child would have 15 and the right 5, then $w_{left} = \\frac{15}{20}$ and $w_{right} = \\frac{5}{20}$.\n",
    "\n",
    "### Exercise 7\n",
    "\n",
    "Implement the information gain function in the cell below. To verify if you have done it correctly you can split ```X_train``` and ```y_train``` using the function you wrote for exercise 5 using node A and B. If correct, the information gain using node A should be higher than when using node B. The function should take the list of labels in the left branch and a list of labels in the right branch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22600024438491684\n",
      "0.10224356360985082\n"
     ]
    }
   ],
   "source": [
    "def IG(left, right):\n",
    "    parent = left + right\n",
    "    w_left = len(left) / len(parent)\n",
    "    w_right = len(right) / len(parent)\n",
    "    return entropy(parent) - w_left * entropy(left) - w_right * entropy(right)   \n",
    "\n",
    "L, yL, R, yR = split(0, 'overcast', X_train, y_train)\n",
    "print(IG(yL,yR))\n",
    "\n",
    "L, yL, R, yR = split(0, 'sunny', X_train, y_train)\n",
    "print(IG(yL,yR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have all the building blocks we need to start fitting a tree to a dataset. Let's give it a go!\n",
    "\n",
    "## Advanced\n",
    "\n",
    "### Exercise 8\n",
    "\n",
    "Implement the ```fit(X,y)``` function below, where X is a matrix of features and y is a list of labels. It should return a tree (i.e., a instance of the Node() class).\n",
    "\n",
    "**Hints:** \n",
    "- Start by thinking of what the right base case is, and implement this.\n",
    "- Remember that this should be a recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if features[0] != \"overcast\" then:\n",
      " \tif features[2] != \"normal\" then:\n",
      " \t\tif features[0] != \"rainy\" then:\n",
      " \t\t\tPredict: \"no\" \n",
      "\t\telse:\n",
      " \t\t\tif features[3] != \"TRUE\" then:\n",
      " \t\t\t\tPredict: \"yes\" \n",
      "\t\t\telse:\n",
      " \t\t\t\tPredict: \"no\" \n",
      "\telse:\n",
      " \t\tif features[3] != \"TRUE\" then:\n",
      " \t\t\tPredict: \"yes\" \n",
      "\t\telse:\n",
      " \t\t\tif features[1] != \"mild\" then:\n",
      " \t\t\t\tPredict: \"no\" \n",
      "\t\t\telse:\n",
      " \t\t\t\tPredict: \"yes\" \n",
      "else:\n",
      " \tPredict: \"yes\"\n"
     ]
    }
   ],
   "source": [
    "def fit(X, y):\n",
    "    if entropy(y) == 0:\n",
    "        return Node(predict=majority(y))\n",
    "    else:\n",
    "        qs = question_set(X)\n",
    "        scores = []\n",
    "        for feature, row in enumerate(qs):\n",
    "            for value in row:\n",
    "                _, yleft, _, yright = split(feature, value, X, y)\n",
    "                scores.append((IG(yleft, yright), feature, value))\n",
    "                \n",
    "        bestIG, feature, value = sorted(scores, key = lambda x: x[0])[-1]\n",
    "        \n",
    "        Xleft, yleft, Xright, yright = split( feature, value, X, y)\n",
    "        \n",
    "        left = fit(Xleft, yleft)\n",
    "        right = fit(Xright, yright)\n",
    "        return Node(feature=feature, value=value, left=left, right=right)\n",
    "    \n",
    "decision_tree = fit(X_train, y_train)\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 9\n",
    "\n",
    "Once we have fitted a decision tree we would like to verify how well it works, and use it to predict the label for new samples. Implement the ```predict``` function in the cell below, where ```tree``` is a fitted tree, and ```x``` is one feature vector (a list). It should return a single label, either 'yes' or 'no'.\n",
    "\n",
    "**Hints:**\n",
    "- What is the base case?\n",
    "- Remember that going left or right depends on the decision at each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tData\t\t\tTruth\tPrediction\n",
      "['sunny', 'hot', 'high', 'FALSE'] \t no \t no\n",
      "['sunny', 'hot', 'high', 'TRUE'] \t no \t no\n",
      "['overcast', 'hot', 'high', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'high', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'cool', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'cool', 'normal', 'TRUE'] \t no \t no\n",
      "['overcast', 'cool', 'normal', 'TRUE'] \t yes \t yes\n",
      "['sunny', 'mild', 'high', 'FALSE'] \t no \t no\n",
      "['sunny', 'cool', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'normal', 'FALSE'] \t yes \t yes\n",
      "['sunny', 'mild', 'normal', 'TRUE'] \t yes \t yes\n",
      "['overcast', 'mild', 'high', 'TRUE'] \t yes \t yes\n",
      "['overcast', 'hot', 'normal', 'FALSE'] \t yes \t yes\n",
      "['rainy', 'mild', 'high', 'TRUE'] \t no \t no\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, x):\n",
    "    if tree.isLeaf():\n",
    "        return tree.predict\n",
    "    elif x[tree.feature] != tree.value:\n",
    "        return predict(tree.left, x)\n",
    "    else:\n",
    "        return predict(tree.right, x)\n",
    "\n",
    "print('\\t\\tData\\t\\t\\tTruth\\tPrediction')\n",
    "for row, label in zip(X_train, y_train):\n",
    "    print(row, '\\t', label, '\\t', predict(decision_tree, row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
