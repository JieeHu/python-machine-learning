{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function minimization\n",
    "\n",
    "At this week's lecture we discussed how learning a set of parameters can be treated as the task of minimizing an error function. Scipy privides a number of ways of finding minima of functions. We'll test of one them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.optimize import fmin_bfgs, fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fmin_bfgs` uses the method called \n",
    "[BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm).\n",
    "We need to give it the following arguments:\n",
    "- f: the function to minimize\n",
    "- x0: the initial guess of the argument with respect to which we're minimizing\n",
    "- fprime: the first derivative of f. If we omit it, `fmin_bfgs` will use a numerical approximatiom\n",
    "\n",
    "For example, consider the following polynomial function: $f(x) = x^4 - 10x^3 + x^2 + x - 4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**4 - 10*x**3 + x**2 + x -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f517864c748>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEACAYAAAC3adEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhUoMYiISAVKDCIiUsH/A5WdwDDh/9DOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51786882b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = numpy.linspace(-2,10,1000)\n",
    "pylab.plot(x,f(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the minimum, using 0 as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -4.093250\n",
      "         Iterations: 5\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 8\n",
      "[-0.15101746]\n"
     ]
    }
   ],
   "source": [
    "print(fmin_bfgs(f, x0=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can suppress the messages using disp=False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15101746]\n"
     ]
    }
   ],
   "source": [
    "print(fmin_bfgs(f, x0=0, disp=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1\n",
    "Looks like we got stuck in a local miniumum. Print the minima findable from the following starting points: -2, 0, 2, 6, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staring at -2 found minimum at [-0.15101775]\n",
      "Staring at 0 found minimum at [-0.15101746]\n",
      "Staring at 2 found minimum at [ 7.42815758]\n",
      "Staring at 6 found minimum at [ 7.42815774]\n",
      "Staring at 10 found minimum at [ 7.4281578]\n"
     ]
    }
   ],
   "source": [
    "# .............................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also use `fmin_bfgs` to minimize functions which take vectors rather than single numbers. For example, let's find the $x_1$ and $x_2$ which minimize the function $g(\\mathbf{x}) = x_1^2 + x_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at [1 1] found minimum at [ -1.07505143e-08  -1.07505143e-08]\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    return numpy.sum(x**2)\n",
    "x0 = numpy.array([1,1])\n",
    "print(\"Starting at {} found minimum at {}\".format(x0, fmin_bfgs(g, x0=x0, disp=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2\n",
    "\n",
    "In this exercise we will regress the fourth feature of the iris dataset agains the first three, using error function minimization. First prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_val, y_train, y_val = train_test_split(iris.data[:,:3], iris.data[:,3], test_size=1/3, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error(wb):\n",
    "    '''Returns the error as a function of intercept and coefficients'''\n",
    "    # We'll get the intercept as the fist element of wb, and the coefficients as the rest\n",
    "    b = wb[0]\n",
    "    w = wb[1:]\n",
    "    # complete the function by returning the sum squared error \n",
    "    # .........................................\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a starting point for the intercept and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First item is the intercept b, the rest the coefficients w\n",
    "wb0 = numpy.array([0.0, 0.0, 0.0, 0.0])\n",
    "#................................"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to find the values which minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.860360\n",
      "         Iterations: 5\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 9\n",
      "[-0.27899142 -0.22846333  0.2540934   0.53891499]\n"
     ]
    }
   ],
   "source": [
    "wb_min = fmin_bfgs(error, x0=wb0)\n",
    "print(wb_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well these parameters do on validation data, in terms of mean absolute error and r-squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167757997158\n",
      "0.914864382486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "# ..........................................\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare these results with the classic implementation of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.27899585678465177, array([-0.22846195,  0.25409292,  0.53891441]))\n",
      "0.167758010212\n",
      "0.914864399764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#.........................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Scikit learn provides two classes `SGDRegressor` and `SGDClassifier` which use stochastic gradient descent to carry out linear regression and classification respectively.\n",
    "\n",
    "These models are especially useful in these situations:\n",
    "- with very large datasets\n",
    "- with streaming data (they support online learning)\n",
    "- with datasets with sparse features\n",
    "\n",
    "SGD is sensitive to learning rate and the scale of the features. It's advisable to z-score the features, and to tune the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the dataset of 50,000 songs. The prediction task is to guess the year the song was made based on 90 timbre features extracted from the audio. The year is in the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "songs = numpy.load(\"songs50k.npy\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(songs[:,1:], songs[:,0], test_size=1/3, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3\n",
    "\n",
    "Train and evaluate the SGD classifier on the songs dataset. Take the following steps:\n",
    "- z-score the training and validation features\n",
    "- find a good set of options related to the learning rate and the number of iterations, according to:\n",
    "  - r-squared on validation data \n",
    "  - mean absolute error on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .........................................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('constant', 1e-05, 5, 377.32239506868206, -1280.4988745664509)\n",
      "('constant', 1e-05, 10, 71.318893501736866, -45.55457698212939)\n",
      "('constant', 1e-05, 50, 6.7206660430561289, 0.21006297232402293)\n",
      "('constant', 0.0001, 5, 6.7552439908788076, 0.20507505576959972)\n",
      "('constant', 0.0001, 10, 6.7549449113502904, 0.20769168079868339)\n",
      "('constant', 0.0001, 50, 6.7310788206587731, 0.20899418721668572)\n",
      "('constant', 0.001, 5, 7.075076440525347, 0.15984736302556612)\n",
      "('constant', 0.001, 10, 7.0033483448744525, 0.16013721262387826)\n",
      "('constant', 0.001, 50, 7.0569015788517904, 0.15406307224909199)\n",
      "('constant', 0.01, 5, 90158067960.919235, -1.5148066821158014e+20)\n",
      "('constant', 0.01, 10, 223108229521.78864, -9.6789738275351639e+20)\n",
      "('constant', 0.01, 50, 136967154465.31032, -3.6161857866836083e+20)\n",
      "('constant', 0.1, 5, 3375831007998.2964, -2.094275424205043e+23)\n",
      "('constant', 0.1, 10, 4196318318169.7217, -3.2242088249958225e+23)\n",
      "('constant', 0.1, 50, 3817202538418.0493, -2.905324868082042e+23)\n",
      "('optimal', 1e-05, 5, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 1e-05, 10, 1229495535872.3823, -2.6783939814862331e+22)\n",
      "('optimal', 1e-05, 50, 472354353.89121795, -4231250804720579.0)\n",
      "('optimal', 0.0001, 5, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.0001, 10, 1229495535872.3823, -2.6783939814862331e+22)\n",
      "('optimal', 0.0001, 50, 472354353.89121795, -4231250804720579.0)\n",
      "('optimal', 0.001, 5, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.001, 10, 1229495535872.3823, -2.6783939814862331e+22)\n",
      "('optimal', 0.001, 50, 472354353.89121795, -4231250804720579.0)\n",
      "('optimal', 0.01, 5, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.01, 10, 1229495535872.3823, -2.6783939814862331e+22)\n",
      "('optimal', 0.01, 50, 472354353.89121795, -4231250804720579.0)\n",
      "('optimal', 0.1, 5, 2175674618252.7292, -9.1619072805100201e+22)\n",
      "('optimal', 0.1, 10, 1229495535872.3823, -2.6783939814862331e+22)\n",
      "('optimal', 0.1, 50, 472354353.89121795, -4231250804720579.0)\n",
      "('invscaling', 1e-05, 5, 1790.3683811622066, -28831.300535871447)\n",
      "('invscaling', 1e-05, 10, 1661.0332696007088, -24816.205139499158)\n",
      "('invscaling', 1e-05, 50, 1076.7670320120212, -10428.399967112029)\n",
      "('invscaling', 0.0001, 5, 665.28895242770341, -3981.0546188332687)\n",
      "('invscaling', 0.0001, 10, 314.32436580047181, -888.51222845415236)\n",
      "('invscaling', 0.0001, 50, 8.4163943321568517, 0.044637149376165475)\n",
      "('invscaling', 0.001, 5, 6.9122748681064143, 0.17634949293650803)\n",
      "('invscaling', 0.001, 10, 6.7687057038388909, 0.20410111635370942)\n",
      "('invscaling', 0.001, 50, 6.7234034941315866, 0.21018459677239287)\n",
      "('invscaling', 0.01, 5, 6.8830944511310097, 0.19277270571818739)\n",
      "('invscaling', 0.01, 10, 6.8142072499606829, 0.19739300928885628)\n",
      "('invscaling', 0.01, 50, 6.7571420301203178, 0.2027476069858537)\n",
      "('invscaling', 0.1, 5, 9.8895674835951617, -0.70301046371337628)\n",
      "('invscaling', 0.1, 10, 8.92197922325688, -0.3187128927835543)\n",
      "('invscaling', 0.1, 50, 8.056098387414524, -0.072515537179933975)\n",
      "Best settings according to MAE ('constant', 1e-05, 50, 6.7206660430561289, 0.21006297232402293)\n",
      "Best settings according to R2 ('invscaling', 0.001, 50, 6.7234034941315866, 0.21018459677239287)\n"
     ]
    }
   ],
   "source": [
    "# .....................................\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.4\n",
    "\n",
    "By default SGDRegressor tries to minimize the standard linear regression error function, that is sum of squared error. However this can be changed, via the `loss=` parameter. When `loss='squared_loss'`, sum of squared errors will be used. Other error functions available include [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) (`loss='huber'`). Compared to squared loss, huber focuses less on outliers.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Huber_loss.svg/600px-Huber_loss.svg.png)\n",
    "\n",
    "Repeat the steps from the previous exercise, but include the tuning of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
